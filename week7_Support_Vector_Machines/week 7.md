# week 7

[toc]

## 1 大边界（间距）分类

**优化目标**

从logistic回归到svm   log->cost

损失函数系数的改变不影响argmin函数的值

系数在哪一项前则更关心哪一项的优化

实际上优化目标是类似的



**大边界（间距）的直观理解**

$\theta^Tx$与正负1的比较，而不是与0比较（安全距离因子）->损失函数要求更严格

margin（SVM的决策边界与分类样本的距离）

SVM以一个最大的间距来分类样本

正则化因子C：过大->受异常值影响过大

小->受异常值影响小，不是线性可分时也有不错的结果



**数学原理**

向量内积？
$$
\min_\theta {\frac{1}{2}} \sum_{j=1}^n \theta_j^2
$$
实际上就是参数向量模长的极小化

实际上，决策边界与参数向量正交

而对于$\theta^Tx$的限制下，最大间距的分类方式使得$\theta$可以更小



## 2 核函数

非线性分类

**定义新的特征**

相似性函数

$\ce{Similarity}(x,l^{})=\exp{(-\frac{||x-l||^2}{2\sigma^2})}$

->高斯核函数

距离越大，值越小

$\sigma$大小决定集中程度

**如何获得L？** 训练集，验证集，测试集

->最小化新特征损失，获得参数

减少成本

**SVM参数**

$C=\frac{1}{\lambda}$  

$\sigma^2$  大：特征$f_i$变化更加平滑，欠拟合；小：变化趋势陡峭，过拟合



## 3 SVM应用

别造轮子

liblinear & libsvm

核函数选择：

训练集较小->线性核函数

特征少，训练集大，非线性边界->高斯核函数->选择$\sigma$

先对特征变量进行归一化

满足默塞尔定理（Mercer's Theorem）的核函数：多项式核函数、字符串……

**Logistic VS SVM**

特征多数据量小->log

特征少数据量也不是特别大->高斯核函数SVM

特征少数据量很大->log或不要核函数（线性）的SVM

凸优化问题