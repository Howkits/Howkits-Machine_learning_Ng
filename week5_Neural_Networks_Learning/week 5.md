# week 5

[toc]

## 1 代价函数与反向传播

### 1 代价函数

logistic回归更像是二分类，涉及到神经网络的多分类更像是每个输出都是一个二分类，因此损失是对所有的输出求和

正则项仍然是对所有的$\theta^2$求和但不惩罚偏置$\theta_0$
$$
\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}
$$

### 2 反向传播

利用公式

使用这种算法可以得到代价函数对所有$\theta$的偏导数了

每次进来一个数据，先前向传播，再反向传播得到偏导，使用梯度下降法等更新，再接收下一个数据



## 2 实战反向传播

### 1 展开参数？

把参数从矩阵展开为向量->优化

从向量转化为矩阵->reshape

### 2 梯度检验

反向传播使用梯度下降法，代价函数值在降低但结果会有误差？

使用函数变化值近似这一点的导数->求偏导

反向传播计算梯度更高效

只是验证反向传播的正确性

### 3 随机初始化

初始参数不能设置为0了->得到的参数都一样，单元计算没有不同->需要随机初始化->对称破裂

### 4 整合

单隐藏层，多隐藏层节点数尽量一致

